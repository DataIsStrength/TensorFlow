{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc94ebbb",
   "metadata": {},
   "source": [
    "TensorFlow-自动求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49f4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42eeb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数值：\n",
      " 9.0\n",
      "导数值：\n",
      " 6.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①tf.variable()：用来定义变量，变量是一种特殊的张量，能够被tensorflow的求导机制求导；\n",
    "②tf.GradientTape()：自动求导记录器，在使用时，被微分的函数必须定义在自动求导记录器的下方并缩进；\n",
    "③tf.square()：计算张量每个元素的平方，并且不改变张量的形状，该函数可以用来计算向量的L2范数。\n",
    "'''\n",
    "#定义变量x，并且令x=3\n",
    "x=tf.Variable(initial_value=3.)\n",
    "#计算y=x^2在x=3处的导数\n",
    "with tf.GradientTape() as tape:\n",
    "    #定义函数y=x^2\n",
    "    y=tf.square(x)\n",
    "#y对x求导\n",
    "y_grad=tape.gradient(y,x)     \n",
    "print('函数值：\\n',y.numpy())\n",
    "print('导数值：\\n',y_grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76937d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数值：\n",
      " 125.0\n",
      "L对w的偏导数值：\n",
      " [[ 70.]\n",
      " [100.]]\n",
      "L对b的偏导数值：\n",
      " 30.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "①tf.reduce_sum()：对输入张量的所有元素求和，该函数可以用来计算向量的范数。\n",
    "'''\n",
    "\n",
    "#定义张量\n",
    "#定义常量X和y\n",
    "X=tf.constant([[1.,2.],[3.,4.]])\n",
    "y=tf.constant([[1.],[2.]])\n",
    "#定义变量w和b\n",
    "w=tf.Variable(initial_value=[[1.],[2.]])\n",
    "b=tf.Variable(initial_value=1.)\n",
    "#求L=||Xw+b-y||^2（该函数是向量Xw+b-y的L2范数）在w=(1,2)T,b=1处的偏导数\n",
    "with tf.GradientTape() as tape:\n",
    "    #定义函数L=||Xw+b-y||^2\n",
    "    L=tf.reduce_sum(tf.square(tf.matmul(X,w)+b-y))\n",
    "#L对w和b分别求偏导\n",
    "w_grad,b_grad=tape.gradient(L,[w,b])\n",
    "print('函数值：\\n',L.numpy())\n",
    "print('L对w的偏导数值：\\n',w_grad.numpy())\n",
    "print('L对b的偏导数值：\\n',b_grad.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
